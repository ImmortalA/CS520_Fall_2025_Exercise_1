\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{hyperref}

\geometry{margin=1in}

\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    showstringspaces=false,
    columns=flexible
}

\title{Exercise 1}
\author{Anh Tran \\ CS520}

\begin{document}

\maketitle

\section{Part 1: Prompt Design and Code Generation}

\subsection{LLM Families}

Two distinct model families were selected:

\begin{itemize}
    \item \textbf{Gemini 2.5 Flash} (Google DeepMind) - API via \texttt{models/gemini-2.5-flash}
    \item \textbf{Mistral Large} (Mistral AI) - API via \texttt{mistral-large-latest}
\end{itemize}

\subsection{Problems Selected}

Ten problems from HumanEval+ dataset \cite{humanevalplus}:

\begin{table}[h]
\centering
\small
\begin{tabular}{lll}
\toprule
\textbf{ID} & \textbf{Function} & \textbf{Type} \\
\midrule
HumanEval/0 & has\_close\_elements & List comparison \\
HumanEval/1 & separate\_paren\_groups & String parsing \\
HumanEval/10 & make\_palindrome & String manipulation \\
HumanEval/12 & longest & List operation \\
HumanEval/17 & parse\_music & String to list \\
HumanEval/25 & factorize & Number theory \\
HumanEval/31 & is\_prime & Number theory \\
HumanEval/54 & same\_chars & String comparison \\
HumanEval/61 & correct\_bracketing & Stack algorithm \\
HumanEval/108 & count\_nums & Number processing \\
\bottomrule
\end{tabular}
\caption{Selected problems}
\end{table}

\subsection{Prompting Strategies}

\textbf{Chain-of-Thought (CoT):}
\begin{lstlisting}
Task: Implement the target function in Python.

Chain-of-Thought:
1) Restate the contract and constraints.
2) Reason about edge cases and examples.
3) Identify a correct and simple algorithm.
4) Implement the function.

Output only the function implementation.

Problem: [description]
Function name: [name]
\end{lstlisting}

\textbf{Stepwise Chain-of-Thought (SCoT):}
\begin{lstlisting}
Task: Implement the target function as specified.

Stepwise Chain-of-Thought Plan:
1) Restate the function contract precisely.
2) Identify edge cases and constraints.
3) Outline a step-by-step algorithm.
4) Implement the function in Python.
5) Double-check correctness on edge cases.

Output: Only the function implementation.

Problem: [description]
Function name: [name]
\end{lstlisting}

\subsection{Experimental Setup}

\begin{itemize}
    \item Generated: 40 solutions (10 problems $\times$ 2 families $\times$ 2 strategies)
    \item API Parameters: Temperature 0.7, Max tokens 2000
    \item Evaluation: HumanEval+ test suites with 10-second timeout
    \item Metric: pass@k = $1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}$
\end{itemize}

\subsection{Results}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{llcc}
\toprule
\textbf{Problem} & \textbf{Family/Strategy} & \textbf{Result} & \textbf{pass@1} \\
\midrule
\multirow{4}{*}{humaneval\_0} & gemini/cot & 1/1 & 1.000 \\
& gemini/scot & 1/1 & 1.000 \\
& mistral/cot & 1/1 & 1.000 \\
& mistral/scot & 1/1 & 1.000 \\
\midrule
\multicolumn{4}{c}{... (humaneval\_1, 10, 12, 17, 31, 61 all passed) ...} \\
\midrule
\multirow{4}{*}{\textbf{humaneval\_25}} & gemini/cot & 1/1 & 1.000 \\
& gemini/scot & 1/1 & 1.000 \\
& mistral/cot & \textbf{0/1} & \textbf{0.000} \\
& mistral/scot & \textbf{0/1} & \textbf{0.000} \\
\midrule
\multirow{4}{*}{\textbf{humaneval\_54}} & gemini/cot & 1/1 & 1.000 \\
& gemini/scot & 1/1 & 1.000 \\
& mistral/cot & \textbf{0/1} & \textbf{0.000} \\
& mistral/scot & \textbf{0/1} & \textbf{0.000} \\
\midrule
\multirow{4}{*}{\textbf{humaneval\_108}} & gemini/cot & \textbf{0/1} & \textbf{0.000} \\
& gemini/scot & 1/1 & 1.000 \\
& mistral/cot & \textbf{0/1} & \textbf{0.000} \\
& mistral/scot & \textbf{0/1} & \textbf{0.000} \\
\bottomrule
\end{tabular}
\caption{Results (failures in bold)}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Family} & \textbf{Strategy} & \textbf{Pass} & \textbf{Fail} & \textbf{Rate} \\
\midrule
Gemini & CoT & 9 & 1 & 90\% \\
Gemini & SCoT & 10 & 0 & \textbf{100\%} \\
Mistral & CoT & 7 & 3 & 70\% \\
Mistral & SCoT & 7 & 3 & 70\% \\
\midrule
\textbf{Overall} & & \textbf{33} & \textbf{7} & \textbf{82.5\%} \\
\bottomrule
\end{tabular}
\caption{Summary by family and strategy}
\end{table}

\subsection{Analysis}

\textbf{Key Findings:}
\begin{enumerate}
    \item Gemini (95\%) outperforms Mistral (70\%)
    \item SCoT improves Gemini to 100\% but doesn't help Mistral
    \item 7 failures identified across 3 problems
\end{enumerate}

\textbf{Failure Types:}
\begin{itemize}
    \item humaneval\_25/mistral: Timeout (inefficient algorithm)
    \item humaneval\_54/mistral: Logic error
    \item humaneval\_108: Edge case handling (negative numbers)
\end{itemize}

\newpage
\clearpage
\section{Part 2: Debugging and Iterative Improvement}

From Part 1, I identified 7 failures and selected 3 representative cases for debugging analysis. I used the self-repair strategy, generating corrected solutions via Gemini API with targeted debugging prompts.

\subsection{Summary of Self-Repair Results}

\begin{table}[h]
\centering
\begin{tabular}{llccc}
\toprule
\textbf{Problem} & \textbf{Family} & \textbf{Original} & \textbf{After Repair} & \textbf{Status} \\
\midrule
humaneval\_25 & mistral & 0.000 & 1.000 & FIXED \\
humaneval\_54 & mistral & 0.000 & 1.000 & FIXED \\
humaneval\_108 & gemini & 0.000 & 1.000 & FIXED \\
\bottomrule
\end{tabular}
\caption{Self-repair success: All 3 failures fixed (100\% improvement)}
\end{table}

\subsection{Failure Case 1: humaneval\_25 (factorize) - Algorithmic Inefficiency}

\subsubsection{Original Failed Attempt}

\textbf{Problem Context:}
\begin{itemize}
    \item Problem: HumanEval/25 (factorize)
    \item Task: Return list of prime factors in ascending order
    \item Family: Mistral Large
    \item Strategy: CoT
    \item Error: TIMEOUT (exceeds 10 seconds on large prime inputs)
\end{itemize}

\textbf{Original CoT Prompt (Failed Attempt):}
\begin{lstlisting}
Task: Implement the target function in Python.

Chain-of-Thought:
1) Restate the contract and constraints.
2) Reason about edge cases and examples.
3) Identify a correct and simple algorithm.
4) Implement the function.

Output only the function implementation.

Problem:
def factorize(n: int) -> List[int]:
    """ Return list of prime factors of given integer in 
    the order from smallest to largest.
    >>> factorize(8)
    [2, 2, 2]
    >>> factorize(25)
    [5, 5]
    """

Function name: factorize
\end{lstlisting}

\textbf{Original Failing Code:}
\begin{lstlisting}
from typing import List

def factorize(n: int) -> List[int]:
    if n == 1:
        return []
    factors = []
    divisor = 2
    while n > 1:
        while n % divisor == 0:
            factors.append(divisor)
            n = n // divisor
        divisor += 1  # Problem: increments by 1 indefinitely
    return factors
\end{lstlisting}

\textbf{Analysis - Why the Model Struggled:}
\begin{enumerate}
    \item \textbf{Reasoning Gap}: Mistral identified trial division but missed the fundamental optimization that divisors beyond $\sqrt{n}$ are impossible (if $d > \sqrt{n}$ and $d|n$, then $n/d < \sqrt{n}$ would have been found earlier).
    
    \item \textbf{Poor Algorithmic Knowledge}: The model knows "check if divisible and divide" but doesn't know the mathematical insight that speeds this up.
    
    \item \textbf{No Performance Consideration}: CoT prompt asks for "correct and simple" but doesn't emphasize efficiency. The solution is correct for small inputs but fails on large ones.
\end{enumerate}

\textbf{Debugging Approach:}
\begin{itemize}
    \item Identified the specific test case causing timeout (input: 9999999967)
    \item Diagnosed the algorithmic bottleneck (no sqrt bound)
    \item Crafted targeted prompt with explicit optimization hint
\end{itemize}

\subsubsection{Improved Attempt with Self-Repair}

\textbf{Self-Repair Prompt Used:}
\begin{lstlisting}
You are given a failing function implementation.

Problem: Return prime factors of integer in order.

Failing code:
[code above]

Issue: TIMEOUT on large primes. The algorithm increments 
divisor by 1 indefinitely. For prime input 9999999967, it 
loops billions of times.

Fix needed: Add sqrt(n) optimization and handle remaining 
prime factor.

Output ONLY the corrected function code.
\end{lstlisting}

\textbf{Fixed Code (Generated by Gemini):}
\begin{lstlisting}
from typing import List

def factorize(n: int) -> List[int]:
    if n == 1:
        return []
    factors = []
    while n % 2 == 0:  # Handle 2 separately
        factors.append(2)
        n //= 2
    divisor = 3
    while divisor * divisor <= n:  # Only check up to sqrt(n)
        while n % divisor == 0:
            factors.append(divisor)
            n //= divisor
        divisor += 2  # Skip even numbers
    if n > 1:  # Remaining n is a prime factor
        factors.append(n)
    return factors
\end{lstlisting}

\textbf{How the Fix Works:}
\begin{itemize}
    \item Checks divisors only up to $\sqrt{n}$ using \texttt{divisor * divisor <= n}
    \item If $n > 1$ remains after loop, it's a prime factor
    \item For 9999999967: checks up to ~100,000 instead of 10 billion
    \item Skips even numbers after 2 (\texttt{divisor += 2})
    \item Time complexity: $O(\sqrt{n})$ instead of $O(n)$
\end{itemize}

\textbf{Lessons Learned:}
\begin{itemize}
    \item \textbf{Effective}: Concrete failing inputs (9999999967) immediately revealed the bottleneck
    \item \textbf{Effective}: Explicitly naming the needed optimization (sqrt) guided the repair
    \item \textbf{Limitation}: The original CoT prompt emphasized correctness but not efficiency, which may have led to the naive implementation
\end{itemize}

\textbf{Result:} pass@1 improved from 0.000 to 1.000 (100\% fix success)

\subsection{Failure Case 2: humaneval\_54 (same\_chars) - Logic Error}

\subsubsection{Original Failed Attempt}

\textbf{Problem:} Check if two strings have the same characters

\textbf{Problem Context:}
\begin{itemize}
    \item Problem: HumanEval/54 (same\_chars)
    \item Task: Check if two words have the same characters
    \item Family: Mistral Large
    \item Strategy: CoT
    \item Error: Incorrect comparison logic (Counter vs set)
\end{itemize}

\textbf{Original CoT Prompt (Failed Attempt):}
\begin{lstlisting}
Task: Implement the target function in Python.

Chain-of-Thought:
1) Restate the contract and constraints.
2) Reason about edge cases and examples.
3) Identify a correct and simple algorithm.
4) Implement the function.

Output only the function implementation.

Problem:
def same_chars(s0: str, s1: str):
    """
    Check if two words have the same characters.
    >>> same_chars('eabcdzzzz', 'dddzzzzzzzddeddabc')
    True
    >>> same_chars('abcd', 'dddddddabc')
    True
    """

Function name: same_chars
\end{lstlisting}

\textbf{Original Failing Code:}
\begin{lstlisting}
from collections import Counter

def same_chars(s0: str, s1: str) -> bool:
    return Counter(s0) == Counter(s1)
\end{lstlisting}

\textbf{Analysis - Why the Model Struggled:}
\begin{enumerate}
    \item \textbf{Misunderstanding of Problem}: Mistral interpreted "same characters" as "same character frequencies" rather than "same set of unique characters."
    
    \item \textbf{Over-Engineering}: Using \texttt{Counter} is more complex than needed. The model chose a sophisticated data structure when a simple \texttt{set} suffices.
    
    \item \textbf{Example Ambiguity}: The examples show strings with different lengths ('eabcdzzzz' vs 'dddzzzzzzzddeddabc'), which should have hinted that frequency doesn't matter, but Mistral missed this.
\end{enumerate}

\textbf{Test Failure Details:}
\begin{itemize}
    \item Test: \texttt{same\_chars('eabcdzzzz', 'dddzzzzzzzddeddabc')}
    \item Expected: True (both have \{e,a,b,c,d,z\})
    \item Actual: False (different 'z' counts: 4 vs 8)
    \item \texttt{Counter('eabcdzzzz')} = \{'e':1, 'a':1, 'b':1, 'c':1, 'd':1, 'z':4\}
    \item \texttt{Counter('dddzzzzzzzddeddabc')} = \{'d':5, 'z':8, 'e':1, 'a':1, 'b':1, 'c':1\}
    \item These Counters are not equal!
\end{itemize}

\textbf{Debugging Approach:}
\begin{itemize}
    \item Analyzed the Counter vs set difference
    \item Provided concrete failing example showing frequency mismatch
    \item Explicitly clarified the unique characters requirement
\end{itemize}

\subsubsection{Improved Attempt with Self-Repair}

\textbf{Self-Repair Prompt Used:}
\begin{lstlisting}
Failing code uses Counter which compares frequency.

Problem states: "Check if two words have the same characters"
Example: ('eabcdzzzz', 'dddzzzzzzzddeddabc') should return True

Both strings contain {e,a,b,c,d,z} but different frequencies.
Counter returns False because frequency differs.

Fix: Use set comparison instead of Counter.

Output ONLY the corrected function code.
\end{lstlisting}

\textbf{Fixed Code (Generated by Gemini):}
\begin{lstlisting}
def same_chars(s0: str, s1: str) -> bool:
    return set(s0) == set(s1)
\end{lstlisting}

\textbf{How the Fix Works:}
\begin{itemize}
    \item \texttt{set(s0)} extracts unique characters only
    \item Frequency is ignored, only presence matters
    \item 'eabcdzzzz' $\rightarrow$ \{e,a,b,c,d,z\}
    \item 'dddzzzzzzzddeddabc' $\rightarrow$ \{e,a,b,c,d,z\}
    \item Sets are equal $\rightarrow$ True
\end{itemize}

\textbf{Lessons Learned:}
\begin{itemize}
    \item \textbf{Effective}: Showing the Counter frequency mismatch clarified the misunderstanding
    \item \textbf{Effective}: Direct suggestion (use set instead) resolved the issue immediately
    \item \textbf{Limitation}: The original examples didn't explicitly state to ignore character frequency, leading to Mistral's misinterpretation
\end{itemize}

\textbf{Result:} pass@1 improved from 0.000 to 1.000

\subsection{Failure Case 3: humaneval\_108 (count\_nums) - Edge Case Handling}

\subsubsection{Original Failed Attempt}

\textbf{Problem:} Count numbers with positive digit sum (negative number handling)

\textbf{Problem Context:}
\begin{itemize}
    \item Problem: HumanEval/108 (count\_nums)
    \item Task: Count numbers with digit sum $> 0$ (negative numbers have first signed digit)
    \item Family: Gemini 2.5 Flash
    \item Strategy: CoT
    \item Error: String parsing bug for negative numbers
\end{itemize}

\textbf{Original CoT Prompt (Failed Attempt):}
\begin{lstlisting}
Task: Implement the target function in Python.

Chain-of-Thought:
1) Restate the contract and constraints.
2) Reason about edge cases and examples.
3) Identify a correct and simple algorithm.
4) Implement the function.

Output only the function implementation.

Problem:
def count_nums(arr):
    """
    Count numbers with sum of digits > 0.
    For negative: first signed digit is negative.
    e.g. -123 has digits -1, 2, 3.
    >>> count_nums([-1, 11, -11]) == 1
    """

Function name: count_nums
\end{lstlisting}

\textbf{Original Failing Code:}
\begin{lstlisting}
def count_nums(arr):
    count = 0
    for num in arr:
        s_num = str(num)
        if num < 0:
            # ERROR: int(s_num[0]) tries to parse "-" character
            digit_sum += int(s_num[0])  # ValueError or wrong logic
            for i in range(1, len(s_num)):
                digit_sum += int(s_num[i])
        else:
            for digit_char in s_num:
                digit_sum += int(digit_char)
        if digit_sum > 0:
            count += 1
    return count
\end{lstlisting}

\textbf{Analysis - Why the Model Struggled:}
\begin{enumerate}
    \item \textbf{Edge Case Misunderstanding}: Gemini attempted to handle negative numbers but made a string indexing error. \texttt{str(-123)[0]} returns '-', not a digit.
    
    \item \textbf{Poor Error Handling}: The code tries \texttt{int(s\_num[0])} which would raise \texttt{ValueError} on '-' character, causing test failure.
    
    \item \textbf{Complex Specification}: The requirement "first signed digit is negative" is subtle. For -123, you need to skip index 0 ('-'), make index 1 negative, and keep indices 2+ positive.
\end{enumerate}

\textbf{Test Failure Details:}
\begin{itemize}
    \item Input: \texttt{count\_nums([-1, 11, -11])}
    \item Expected: 1 (only 11 has positive digit sum)
    \item For -1: digit sum = -1 (not $> 0$)
    \item For 11: digit sum = 1 + 1 = 2 ($> 0$) ✓
    \item For -11: digit sum = -1 + 1 = 0 (not $> 0$)
    \item Gemini's code likely counted all 3 or crashed
\end{itemize}

\textbf{Debugging Approach:}
\begin{itemize}
    \item Traced through the string indexing error
    \item Provided explicit example showing correct index handling
    \item Included step-by-step calculation for the failing test case
\end{itemize}

\subsubsection{Improved Attempt with Self-Repair}

\textbf{Self-Repair Prompt Used:}
\begin{lstlisting}
Failing code has bug with negative numbers.

For -123, str(-123)[0] is '-' character, not a digit.
Should parse: digit 1 as negative (-1), then 2, 3 as positive.
Result: -1 + 2 + 3 = 4 > 0

Test case:
count_nums([-1, 11, -11]) should return 1
-1: -1 (sum=-1, not > 0)
11: 1+1=2 (sum=2, > 0) ✓
-11: -1+1=0 (sum=0, not > 0)

Output ONLY the corrected function code.
\end{lstlisting}

\textbf{Fixed Code (Generated by Gemini):}
\begin{lstlisting}
def count_nums(arr):
    count = 0
    for num in arr:
        s_num = str(num)
        if num < 0:
            # s_num[1] is first digit (make it negative)
            digit_sum = -int(s_num[1])
            # s_num[2:] are remaining digits (positive)
            for i in range(2, len(s_num)):
                digit_sum += int(s_num[i])
        else:
            digit_sum = sum(int(d) for d in s_num)
        if digit_sum > 0:
            count += 1
    return count
\end{lstlisting}

\textbf{How the Fix Works:}
\begin{itemize}
    \item Skip '-' sign at index 0
    \item \texttt{s\_num[1]} is first digit, negate it: \texttt{-int(s\_num[1])}
    \item \texttt{s\_num[2:]} are remaining digits, sum positive
    \item -123: -int('1') + int('2') + int('3') = -1 + 2 + 3 = 4
\end{itemize}

\textbf{Lessons Learned:}
\begin{itemize}
    \item \textbf{Effective}: Detailed string index breakdown made the fix obvious
    \item \textbf{Effective}: Concrete calculation (-123 → -1+2+3) showed the correct approach
    \item \textbf{Observation}: Gemini's SCoT already succeeded on this problem, suggesting that structured prompting can prevent such errors. The self-repair essentially recovered what SCoT achieved through better initial prompting.
\end{itemize}

\textbf{Result:} pass@1 improved from 0.000 to 1.000

\subsection{Cross-Family Comparison}

\textbf{Gemini vs Mistral Failure Patterns:}

\begin{itemize}
    \item \textbf{Gemini}: Failed on 1 problem (humaneval\_108 CoT only). SCoT strategy succeeded on the same problem, showing prompting strategy matters.
    
    \item \textbf{Mistral}: Failed on 3 problems (humaneval\_25, humaneval\_54, humaneval\_108). Neither CoT nor SCoT helped, indicating fundamental reasoning gaps.
    
    \item \textbf{Algorithmic Knowledge}: Gemini knew sqrt optimization for factorize; Mistral did not.
    
    \item \textbf{Edge Case Handling}: Both struggled with negative number edge cases, but Gemini's SCoT succeeded while Mistral's did not.
\end{itemize}

\textbf{Debugging Improvements Differ in Effectiveness:}

\begin{table}[h]
\centering
\begin{tabular}{llcc}
\toprule
\textbf{Problem} & \textbf{Family} & \textbf{Repair Model} & \textbf{Success} \\
\midrule
humaneval\_25 & Mistral (failed) & Gemini & Yes (1.000) \\
humaneval\_54 & Mistral (failed) & Gemini & Yes (1.000) \\
humaneval\_108 & Gemini (failed) & Gemini & Yes (1.000) \\
\bottomrule
\end{tabular}
\caption{All repairs performed by Gemini API}
\end{table}

\textbf{Key Observations:}
\begin{enumerate}
    \item \textbf{Cross-Model Repair}: Gemini successfully repaired Mistral's failures, showing that a more capable model can debug another model's code when given specific feedback.
    
    \item \textbf{Self-Repair Capability}: Gemini can also fix its own mistakes (humaneval\_108), demonstrating that targeted feedback helps even on self-generated code.
    
    \item \textbf{100\% Success Rate}: All 3 repairs succeeded, suggesting self-repair with concrete test cases is highly reliable.
    
    \item \textbf{Different Failure Types, Same Solution Approach}: Whether the issue is algorithmic (factorize), logical (same\_chars), or edge-case (count\_nums), providing exact failure feedback works consistently.
\end{enumerate}

\textbf{Self-Repair Effectiveness:}

All 3 failures were successfully repaired to 100\% pass@1, demonstrating that:
\begin{enumerate}
    \item Self-repair prompting with specific failure feedback is highly effective across different error types
    \item Even a "smarter" model (Gemini) can fix a "weaker" model's (Mistral) mistakes when given precise debugging hints
    \item Providing exact test cases and expected behavior guides LLMs to correct solutions
    \item The self-repair strategy is robust: it worked on timeout issues, logic errors, and edge case bugs
\end{enumerate}

\subsection{Part 2 Conclusion}

The self-repair strategy successfully fixed all 3 failures (100\% success rate):
\begin{itemize}
    \item humaneval\_25: Algorithmic efficiency restored through sqrt optimization
    \item humaneval\_54: Logic error corrected by switching from Counter to set comparison
    \item humaneval\_108: Edge case handling repaired with proper string indexing
\end{itemize}

This experiment demonstrates that targeted debugging feedback with specific test cases and error descriptions enables LLMs to generate correct solutions even when initial prompting strategies fail. The approach is particularly effective because it provides concrete failure information rather than generic improvement requests.

\newpage
\clearpage
\section{Part 3: Innovation - Test-Driven Refinement Strategy}

\subsection{Novel Strategy Description}

\textbf{Strategy Name:} Test-Driven Refinement

\textbf{Motivation:} 

From Part 1 and Part 2 analysis, I observed that many failures stemmed from inadequate edge case handling (humaneval\_108) or missing optimizations (humaneval\_25). While CoT and SCoT ask models to "consider edge cases," they don't enforce it. I designed Test-Driven Refinement to explicitly require edge case enumeration and mental verification before code generation.

\textbf{Key Innovation:}

Rather than treating edge cases as an afterthought, this strategy makes them the primary design driver:
\begin{enumerate}
    \item Explicitly list ALL edge cases with expected behavior
    \item Design algorithm specifically to handle each case
    \item Mentally trace through implementation for each case
    \item Revise if any case would fail
    \item Only output after verification
\end{enumerate}

This mimics test-driven development but without writing actual test code - the model must internalize test thinking.

\textbf{Specific Example Application:} For humaneval\_108 (count\_nums), the strategy explicitly enumerated edge cases like:
\begin{itemize}
    \item Empty list: return 0
    \item Single element: return 1 if positive, 0 if negative/zero
    \item Mixed positive/negative: count only positive numbers
    \item All negative: return 0
\end{itemize}

However, this explicit enumeration led to overly complex implementations that failed on basic test cases, suggesting the models struggled with the additional cognitive load.

\subsection{Prompt Template}

\begin{lstlisting}[caption={Test-Driven Refinement Prompt}]
Task: Implement the target function using a test-driven 
refinement approach.

Test-Driven Refinement Strategy:
1) Read the function contract and identify ALL edge cases 
   mentioned or implied.
2) For each edge case, write out what the expected behavior 
   should be (in comments).
3) Design your algorithm specifically to handle each 
   identified edge case.
4) Implement the function in Python.
5) Mentally trace through your implementation with each 
   edge case to verify correctness.
6) If any edge case would fail, revise your implementation 
   before outputting.

Output: Only the final refined function implementation that 
handles all edge cases.

IMPORTANT: Your implementation must be correct on first try. 
Think through edge cases carefully before coding.

Problem:
[problem description]

Function name: [function_name]
\end{lstlisting}

\subsection{Implementation}

Generated 20 solutions (10 problems $\times$ 2 families) using the Test-Driven Refinement strategy via API calls to both Gemini and Mistral.

\subsection{Results}

\subsection{Detailed Results Analysis}

Table \ref{tab:detailed_comparison} shows the complete problem-by-problem comparison across all strategies:

\begin{table}[h]
\centering
\begin{tabular}{|l|l|c|c|c|c|}
\hline
\textbf{Problem} & \textbf{Family} & \textbf{CoT} & \textbf{SCoT} & \textbf{Innovation} & \textbf{Best} \\
\hline
humaneval\_0 & Gemini & 100.0\% & 100.0\% & 0.0\% & CoT \\
humaneval\_0 & Mistral & 100.0\% & 100.0\% & 0.0\% & CoT \\
humaneval\_1 & Gemini & 100.0\% & 100.0\% & 0.0\% & CoT \\
humaneval\_1 & Mistral & 100.0\% & 100.0\% & 0.0\% & CoT \\
humaneval\_10 & Gemini & 100.0\% & 100.0\% & 100.0\% & CoT \\
humaneval\_10 & Mistral & 100.0\% & 100.0\% & 100.0\% & CoT \\
humaneval\_108 & Gemini & 0.0\% & 100.0\% & 100.0\% & SCoT \\
humaneval\_108 & Mistral & 0.0\% & 0.0\% & 0.0\% & CoT \\
humaneval\_12 & Gemini & 100.0\% & 100.0\% & 100.0\% & CoT \\
humaneval\_12 & Mistral & 100.0\% & 100.0\% & 0.0\% & CoT \\
humaneval\_17 & Gemini & 100.0\% & 100.0\% & 100.0\% & CoT \\
humaneval\_17 & Mistral & 100.0\% & 100.0\% & 0.0\% & CoT \\
humaneval\_25 & Gemini & 100.0\% & 100.0\% & 0.0\% & CoT \\
humaneval\_25 & Mistral & 0.0\% & 0.0\% & 0.0\% & CoT \\
humaneval\_31 & Gemini & 100.0\% & 100.0\% & 100.0\% & CoT \\
humaneval\_31 & Mistral & 100.0\% & 100.0\% & 100.0\% & CoT \\
humaneval\_54 & Gemini & 100.0\% & 100.0\% & 100.0\% & CoT \\
humaneval\_54 & Mistral & 0.0\% & 0.0\% & 0.0\% & CoT \\
humaneval\_61 & Gemini & 100.0\% & 100.0\% & 100.0\% & CoT \\
humaneval\_61 & Mistral & 100.0\% & 100.0\% & 100.0\% & CoT \\
\hline
\end{tabular}
\caption{Detailed problem-by-problem strategy comparison}
\label{tab:detailed_comparison}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Innovation never outperformed}: Test-Driven Refinement never achieved better results than the best baseline strategy
    \item \textbf{Complete failures}: Innovation failed completely (0.0\%) on 6 out of 20 problem-family combinations
    \item \textbf{Only tie successes}: Innovation only succeeded where baselines also succeeded, never improving upon baseline failures
    \item \textbf{Consistent pattern}: Gemini consistently outperformed Mistral across all strategies
\end{itemize}

\subsection{Innovation Strategy Impact Analysis}

The quantitative impact analysis reveals significant performance degradation:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Family} & \textbf{Best Baseline} & \textbf{Innovation} & \textbf{Change} & \textbf{Change \%} \\
\hline
Gemini & 100.0\% (SCoT) & 70.0\% & -30.0\% & -30.0\% \\
Mistral & 70.0\% (CoT/SCoT) & 30.0\% & -40.0\% & -57.1\% \\
\hline
\end{tabular}
\caption{Innovation strategy impact analysis}
\end{table}

\textbf{Critical Findings:}
\begin{itemize}
    \item \textbf{Gemini degradation}: 30\% absolute decline from perfect SCoT performance
    \item \textbf{Mistral degradation}: 57\% relative decline from baseline performance
    \item \textbf{No improvement cases}: Innovation never improved upon baseline failures
    \item \textbf{Family-specific impact}: Mistral suffered more severe degradation than Gemini
\end{itemize}

\subsection{Analysis}

\textit{After generating and evaluating, analyze:}

\textbf{Did it improve results?}

The Test-Driven Refinement strategy did not improve results compared to existing strategies. Overall performance declined significantly:

\begin{itemize}
    \item \textbf{Overall decline}: Innovation achieved 50.0\% average pass@1 vs 85.0\% for SCoT (best baseline)
    \item \textbf{Gemini impact}: 70.0\% vs 100.0\% SCoT (-30.0\% decline)
    \item \textbf{Mistral impact}: 30.0\% vs 70.0\% baseline (-57.1\% decline)
    \item \textbf{Specific failures}: 
    \begin{itemize}
        \item humaneval\_0 (has\_close\_elements): Both families failed (0.0\% vs 100.0\% baseline)
        \item humaneval\_1 (separate\_paren\_groups): Both families failed (0.0\% vs 100.0\% baseline)
        \item humaneval\_25 (factorize): Gemini failed (0.0\% vs 100.0\% baseline)
        \item humaneval\_108 (count\_nums): Mistral failed (0.0\% vs 0.0\% baseline, but Gemini succeeded)
    \end{itemize}
\end{itemize}

The strategy performed worse than both CoT and SCoT across all families, suggesting that explicit edge case enumeration may have introduced complexity that hindered rather than helped the models. The failures were particularly pronounced on problems involving mathematical operations and string manipulation.

\textbf{Hypothesis Testing:}

My hypothesis was that explicit edge case enumeration would improve performance on complex problems like humaneval\_108. \textbf{This hypothesis was refuted.} The Test-Driven Refinement strategy failed on humaneval\_108 for Mistral (0.0\% vs 0.0\% baseline) and only succeeded for Gemini where SCoT already achieved 100\% success. The explicit edge case focus may have overcomplicated the reasoning process, leading to more errors rather than fewer.

\textbf{Differences Across Families:}

Gemini showed better resilience to the Test-Driven Refinement strategy than Mistral:

\begin{itemize}
    \item \textbf{Gemini}: 70.0\% success rate (7/10) vs Mistral's 30.0\% (3/10)
    \item \textbf{Consistent pattern}: This mirrors the CoT vs SCoT results where Gemini generally outperformed Mistral
    \item \textbf{Possible reasons}: Gemini may be better at handling complex, structured prompts, while Mistral struggled with the additional cognitive load of explicit edge case enumeration
    \item \textbf{Family characteristics}: Gemini's superior performance suggests it may be more robust to prompt complexity, while Mistral performs better with simpler, more direct instructions
\end{itemize}

\textbf{Lessons Learned:}

The Test-Driven Refinement experiment provided valuable insights despite its failure:

\begin{itemize}
    \item \textbf{What assumptions were correct}: Edge cases are important for robust solutions
    \item \textbf{What assumptions were wrong}: Explicit enumeration helps models - it actually hindered performance
    \item \textbf{What could be improved}: Simpler prompts focusing on natural reasoning rather than forced structure
    \item \textbf{Would this scale to harder problems}: Unlikely, as the strategy failed even on medium-difficulty problems
\end{itemize}

\subsection{Part 3 Conclusion}

The Test-Driven Refinement innovation strategy was not successful, achieving only 50.0\% overall pass@1 compared to 85.0\% for the best baseline (SCoT). While the strategy was theoretically sound, the explicit edge case enumeration approach proved counterproductive, likely due to increased cognitive load and over-constraining the models' natural reasoning processes.

The experiment revealed important insights about prompt design: complex, structured approaches may not always improve performance, and different LLM families respond differently to prompt complexity. Future innovations should focus on simpler, more natural reasoning patterns that work across multiple model families.

Despite the failure, this experiment contributes to understanding the boundaries of effective prompting strategies and provides valuable data for future research in LLM code generation.

\section{Overall Conclusion}

\textbf{Summary:}
\begin{itemize}
    \item Part 1: 82.5\% success (33/40), Gemini superior
    \item Part 2: [X] failures debugged successfully
    \item Part 3: [Innovation result]
\end{itemize}

\textbf{Key Insights:}
\begin{itemize}
    \item Structured prompting helps capable models
    \item Algorithmic knowledge varies between families
    \item [Your insight from innovation]
\end{itemize}

\section{Repository}

All code, prompts, tests, and results:\\
\url{https://github.com/ImmortalA/CS520_Fall_2025_Exercise_1}

\begin{thebibliography}{9}
\bibitem{humanevalplus}
EvalPlus Team. HumanEval+ Dataset. 
\url{https://huggingface.co/datasets/evalplus/humanevalplus}, 2023.
\end{thebibliography}

\end{document}

